{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "\n",
    "\n",
    "def load_file(filename):\n",
    "    with open('transcripts.txt',\"r\") as transcript_file:\n",
    "        transcript = transcript_file.read()\n",
    "    transcript_file.close()\n",
    "    return transcript\n",
    "\n",
    "def clean_doc(file):\n",
    "    file = file.replace(\"\\n,\", \"[\").replace('\",,' , \"\\n\\n\\n\\n\").replace(\":\" , \"]:\").replace(\"[[\", \"[\").replace(\",\" , \"\").replace(\"  \" , \" \").replace(\"\\n[\\n\", \"\").replace(\"\\t\", \" \").replace(\"'\", \"\")\n",
    "    file = file.replace('\"', \"\")\n",
    "    tokens = file.split()\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return file\n",
    "\n",
    "def punct_replace(file):\n",
    "    file = file.replace('\\n', \" NEWLINE \").replace( '!', \" EXCLAMATIONMARK\").replace(\"$\", \"DOLLARSIGN \")\n",
    "    file = file.replace(\".\", \" FULLSTOP \").replace(\"/\", \" FORWARDSLASH \").replace(\":\", \" COLON\")\n",
    "    file = file.replace(\"?\", \" QUESTIONMARK\").replace(\"[\", \"LEFTSQUAREBRACKET \").replace(\"]\", \" RIGHTSQUAREBRACKET\")\n",
    "    return file\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    word_counts = Counter(text)\n",
    "    \n",
    "    word_count = {k:word_counts[k] for k in word_counts if word_counts[k] > 10}\n",
    "    ignored_words = {k:word_counts[k] for k in word_counts if word_counts[k] < 10}\n",
    "    \n",
    "    sorted_vocab = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    sorted_ignored = sorted(ignored_words, key=ignored_words.get)\n",
    "        \n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    return vocab_to_int, int_to_vocab, sorted_ignored\n",
    "    \n",
    "\n",
    "def sentences(sorted_ignored, all_words):\n",
    "    SEQUENCE_LENGTH = 10\n",
    "    sentences = []\n",
    "    next_words = []\n",
    "    ignored = 0\n",
    "    for i in range(0, len(all_words) - SEQUENCE_LENGTH, 1):\n",
    "        if len(set(all_words[i:i+SEQUENCE_LENGTH+1]).intersection(sorted_ignored)) == 0:\n",
    "            sentences.append(all_words[i:i+SEQUENCE_LENGTH])\n",
    "            next_words.append(all_words[i+SEQUENCE_LENGTH])\n",
    "        else:\n",
    "            ignored = ignored+1\n",
    "    print('Ignored sequences: ', ignored)\n",
    "    print('Remaining sequences: ', len(sentences))\n",
    "    return sentences, next_words\n",
    "\n",
    "\n",
    "in_filename = 'transcripts.txt'\n",
    "in_file = load_file(in_filename)\n",
    "\n",
    "cleaned_file = clean_doc(in_file)\n",
    "\n",
    "punct_replaced = punct_replace(cleaned_file)\n",
    "punct_replaced = punct_replaced.lower()\n",
    "punct_replaced = punct_replaced.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab, sorted_ignored = create_lookup_tables(punct_replaced)\n",
    "sentences, next_words = sentences(sorted_ignored, punct_replaced)\n",
    "\n",
    "\n",
    "\n",
    "#corpus = sorted(list(set(clean_file)))\n",
    "#print(corpus)\n",
    "\n",
    "#print('Dataset Stats')\n",
    "#print('Roughly the number of unique words: {}'.format(len({word: None for word in cleaned_file.split()})))\n",
    "#scenes = cleaned_file.split('\\n\\n\\n\\n\\n\\n')\n",
    "#print('Number of episodes: {}'.format(len(scenes)))\n",
    "#sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "#print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "#sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "#print('Number of lines: {}'.format(len(sentences)))\n",
    "#word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "#print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "#print()\n",
    "#print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "#print('\\n'.join(cleaned_file.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 720758\n",
      "Size of test set = 180190\n"
     ]
    }
   ],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=20):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "sentences, next_words, sentences_test, next_words_test = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Bidirectional, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "\n",
    "def get_model(dropout=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(128), input_shape=(10, len(punct_replaced))))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    \n",
    "    model.add(Dense(len(punct_replaced)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "    \n",
    "#x = np.zeros((len(sentences), 10, len(punct_replaced)), dtype=np.bool)\n",
    "#y = np.zeros((len(sentences), len(punct_replaced)), dtype=np.bool)\n",
    "\n",
    "word_indices=dict((c, i) for i, c in enumerate(punct_replaced))\n",
    "indicied_word=dict((i, c) for i, c in enumerate(punct_replaced))\n",
    "\n",
    "\n",
    "def generator(sentences, next_words, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, 10, len(punct_replaced)), dtype=np.bool)\n",
    "        y = np.zeros((batch_size, len(punct_replaced)), dtype=np.bool)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentences[index]):\n",
    "                x[i, t, word_indices[w]] = 1\n",
    "                y[i, word_indices[next_words[index]]] = 1\n",
    "                \n",
    "                index = index + 1\n",
    "                if index == len(sentences):\n",
    "                    index = 0\n",
    "            yield x, y\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit_generator(generator(sentences, next_words, 32), \n",
    "                    steps_per_epoch=int(len(sentences)/32) + 1, \n",
    "                                        epochs=100, \n",
    "                                       validation_data=generator(sentences_test, next_words_test, 32),\n",
    "                                       validation_steps=int(len(sentences_test)/32)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
