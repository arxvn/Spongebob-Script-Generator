{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of episodes: 418\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_file(filename):\n",
    "    with open('transcripts.txt',\"r\") as transcript_file:\n",
    "        transcript = transcript_file.read()\n",
    "    transcript_file.close()\n",
    "    return transcript\n",
    "\n",
    "def clean_doc(file):\n",
    "    file = file.replace(\"\\n,\", \"[\").replace('\",,' , \"\\n\\n\\n\\n\").replace(\":\" , \"]:\").replace(\"[[\", \"[\").replace(\",\" , \"\").replace(\"  \" , \" \").replace(\"\\n[\\n\", \"\").replace(\"\\t\", \" \").replace(\"'\", \"\")\n",
    "    file = file.replace('\"', \"\")\n",
    "    tokens = file.split()\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return file\n",
    "\n",
    "def punct_replace(file):\n",
    "    file = file.replace('\\n', \" NEWLINE \").replace( '!', \" EXCLAMATIONMARK\").replace(\"$\", \"DOLLARSIGN \")\n",
    "    file = file.replace(\".\", \" FULLSTOP \").replace(\"/\", \" FORWARDSLASH \").replace(\":\", \" COLON\")\n",
    "    file = file.replace(\"?\", \" QUESTIONMARK\").replace(\"[\", \"LEFTSQUAREBRACKET \").replace(\"]\", \" RIGHTSQUAREBRACKET\")\n",
    "    return file\n",
    "\n",
    "\n",
    "in_filename = 'transcripts.txt'\n",
    "in_file = load_file(in_filename)\n",
    "\n",
    "cleaned_file = clean_doc(in_file)\n",
    "\n",
    "punct_replaced = punct_replace(cleaned_file)\n",
    "punct_replaced = punct_replaced.lower()\n",
    "words_in_text = punct_replaced.split()\n",
    "\n",
    "\n",
    "\n",
    "#corpus = sorted(list(set(clean_file)))\n",
    "#print(corpus)\n",
    "\n",
    "#print('Dataset Stats')\n",
    "#print('Roughly the number of unique words: {}'.format(len({word: None for word in cleaned_file.split()})))\n",
    "scenes = cleaned_file.split('\\n\\n\\n\\n\\n\\n')\n",
    "print('Number of episodes: {}'.format(len(scenes)))\n",
    "#sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "#print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "#sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "#print('Number of lines: {}'.format(len(sentences)))\n",
    "#word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "#print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "#print()\n",
    "#print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "#print('\\n'.join(cleaned_file.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 21091\n",
      "Ignoring words with frequency < 10\n",
      "Unique words after ignoring: 4970\n",
      "Ignored sequences: 467747\n",
      "Remaining sequences: 770290\n",
      "Number of unique words(corpus size) 4970\n"
     ]
    }
   ],
   "source": [
    "    word_freq = {}\n",
    "    for word in words_in_text:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "    ignored_words = set()\n",
    "    for k, v in word_freq.items():\n",
    "        if word_freq[k] < 10:\n",
    "            ignored_words.add(k)\n",
    "    words = set(words_in_text)\n",
    "    print('Unique words before ignoring:', len(words))\n",
    "    print('Ignoring words with frequency <', 10)\n",
    "    words = sorted(set(words) - ignored_words)\n",
    "    print('Unique words after ignoring:', len(words))\n",
    "\n",
    "    word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "    indices_word = dict((i, c) for i, c in enumerate(words))\n",
    "\n",
    "    # cut the text in semi-redundant sequences of SEQUENCE_LEN words\n",
    "    sentences = []\n",
    "    next_words = []\n",
    "    ignored = 0\n",
    "    SEQUENCE_LENGTH = 16\n",
    "\n",
    "    for i in range(0, len(words_in_text) - SEQUENCE_LENGTH, 1):\n",
    "        # Only add the sequences where no word is in ignored_words\n",
    "        if len(set(words_in_text[i: i+SEQUENCE_LENGTH+1]).intersection(ignored_words)) == 0:\n",
    "            sentences.append(words_in_text[i: i + SEQUENCE_LENGTH])\n",
    "            next_words.append(words_in_text[i + SEQUENCE_LENGTH])\n",
    "        else:\n",
    "            ignored = ignored + 1\n",
    "    print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))\n",
    "print('Number of unique words(corpus size):',len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "if 'yagh' in next_words:\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 754884\n",
      "Size of test set = 15406\n"
     ]
    }
   ],
   "source": [
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(sentences, next_words), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print((3 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\aruns\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\aruns\\miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 1024)        5089280   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               1180672   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4970)              1277290   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4970)              0         \n",
      "=================================================================\n",
      "Total params: 7,547,242\n",
      "Trainable params: 7,547,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import losses\n",
    "from keras.layers import Dense, Activation, LSTM, Bidirectional, Dropout, Embedding\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import RMSprop\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def get_model(dropout=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(words), output_dim=1024))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(words)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "    \n",
    "\n",
    "def generator(sentences, next_words, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, 16), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        for i in range(batch_size): \n",
    "            for num, word in enumerate(sentences[index % len(sentences)]):\n",
    "                x[i, num] = word_indices[word]\n",
    "            y[i] = word_indices[next_words[index % len(sentences)]]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "model = get_model()\n",
    "model.summary()\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    in_filename = 'predicted.txt'\n",
    "    f=open(\"predicted.txt\",\"a\")\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(sentences+sentences_test))\n",
    "    seed = (sentences+sentences_test)[seed_index]\n",
    "\n",
    "    for diversity in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    #for diversity in [0.7]:\n",
    "        sentence = seed\n",
    "        f.write('%02d %.3f\\n' % (epoch, logs['loss']))\n",
    "        f.write('\\n ----- Diversity:' + str(diversity) + '\\n')\n",
    "        f.write('----- Generating with seed:\\n\"' + ' '.join(sentence)+ '\\n\\n')\n",
    "        predicted_text = \"\"\n",
    "        for i in range(640):\n",
    "            x_pred = np.zeros((1, 16))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t] = word_indices[word]\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "            predicted_text += (next_word + \" \")\n",
    "        f.write(predicted_text)\n",
    "        f.write('='*80 + '\\n')\n",
    "        file = load_file(in_filename)\n",
    "        cleaned = post_process(file) \n",
    "        w=open(\"cleanpredicted.txt\",\"w+\")\n",
    "        w.write(cleaned)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def load_file(filename):\n",
    "    with open('predicted.txt',\"r\") as transcript_file:\n",
    "        transcript = transcript_file.read()\n",
    "    transcript_file.close()\n",
    "    return transcript\n",
    "\n",
    "\n",
    "\n",
    "def post_process(file):\n",
    "    file = file.replace(\"newline\", '\\n').replace(\"exclamationmark\", '!').replace(\"dollarsign \",\"$\")\n",
    "    file = file.replace(\" fullstop\",\".\").replace(\"forwardslash \",\"/\").replace(\"colon\",\":\")\n",
    "    file = file.replace(\" questionmark\",\"?\").replace(\"leftsquarebracket \",\"[\").replace(\" rightsquarebracket\",\"]\")\n",
    "    return file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "\n",
    "#model.fit_generator(generator(sentences, next_words, BATCH_SIZE), \n",
    " #                   steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1, \n",
    "  #                                      epochs=10, \n",
    "   #                                    validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "    #                                   validation_steps=int(len(sentences_test)/BATCH_SIZE)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "callbacks_list = [print_callback, early_stopping]\n",
    "BATCH_SIZE= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10417/10417 [==============================] - 328s 31ms/step - loss: 3.8092 - acc: 0.3357 - val_loss: 3.6986 - val_acc: 0.3417\n",
      "Epoch 2/100\n",
      "10417/10417 [==============================] - 329s 32ms/step - loss: 3.4945 - acc: 0.3593 - val_loss: 3.6114 - val_acc: 0.3512\n",
      "Epoch 3/100\n",
      "10417/10417 [==============================] - 328s 31ms/step - loss: 3.3153 - acc: 0.3736 - val_loss: 3.5974 - val_acc: 0.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pygpu_labs/envs/Labs_y3/lib/python3.6/site-packages/ipykernel_launcher.py:37: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100\n",
      "10417/10417 [==============================] - 327s 31ms/step - loss: 3.1763 - acc: 0.3855 - val_loss: 3.6064 - val_acc: 0.3611\n",
      "Epoch 5/100\n",
      "10417/10417 [==============================] - 327s 31ms/step - loss: 3.0631 - acc: 0.3959 - val_loss: 3.6404 - val_acc: 0.3556\n",
      "Epoch 6/100\n",
      "10417/10417 [==============================] - 329s 32ms/step - loss: 2.9693 - acc: 0.4050 - val_loss: 3.6641 - val_acc: 0.3553\n",
      "Epoch 7/100\n",
      "10417/10417 [==============================] - 328s 32ms/step - loss: 2.8909 - acc: 0.4127 - val_loss: 3.7045 - val_acc: 0.3552\n",
      "Epoch 8/100\n",
      " 1051/10417 [==>...........................] - ETA: 4:52 - loss: 2.8391 - acc: 0.4195"
     ]
    }
   ],
   "source": [
    "   model.fit_generator(generator(sentences, next_words, BATCH_SIZE),\n",
    "                        steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                        epochs=100,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                        validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
